{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Python Embeddings —á–µ—Ä–µ–∑ Cynosure Bridge\n",
    "\n",
    "–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ embedding —á–µ—Ä–µ–∑ OpenAI API —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Cynosure Bridge\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://192.168.1.196:3000/v1\",\n",
    "    api_key=\"dummy-key\"\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ embedding\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=\"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\"\n",
    ")\n",
    "\n",
    "embedding = response.data[0].embedding\n",
    "print(f\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞: {len(embedding)}\")\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "| OpenAI Model | Claude Alternative | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ |\n",
    "|--------------|-------------------|-------------|------------|\n",
    "| `text-embedding-3-small` | `claude-3-5-sonnet-20241022` | 1536 | –ë—ã—Å—Ç—Ä–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è |\n",
    "| `text-embedding-3-large` | `claude-3-5-sonnet-20241022` | 3072 | –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ |\n",
    "| `text-embedding-ada-002` | `claude-3-5-haiku-20241022` | 1536 | –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, documents):\n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "    query_response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=query\n",
    "    )\n",
    "    query_embedding = query_response.data[0].embedding\n",
    "    \n",
    "    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    doc_embeddings = []\n",
    "    for doc in documents:\n",
    "        doc_response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=doc\n",
    "        )\n",
    "        doc_embeddings.append(doc_response.data[0].embedding)\n",
    "    \n",
    "    # –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–µ–≥–æ\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) for doc_emb in doc_embeddings\n",
    "    ]\n",
    "    \n",
    "    best_match_idx = np.argmax(similarities)\n",
    "    return documents[best_match_idx], similarities[best_match_idx]\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "documents = [\n",
    "    \"Python - —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è\",\n",
    "    \"JavaScript –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏\", \n",
    "    \"Claude - —ç—Ç–æ AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –æ—Ç Anthropic\"\n",
    "]\n",
    "\n",
    "result, score = semantic_search(\"–ß—Ç–æ —Ç–∞–∫–æ–µ Python?\", documents)\n",
    "print(f\"–ù–∞–π–¥–µ–Ω–æ: {result} (score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_embeddings(texts, batch_size=100):\n",
    "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤ —Ç–µ–∫—Å—Ç–∞ –ø–∞–∫–µ—Ç–∞–º–∏\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i + batch_size, len(texts))} –∏–∑ {len(texts)}\")\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "large_text_list = [f\"–¢–µ–∫—Å—Ç –Ω–æ–º–µ—Ä {i}\" for i in range(50)]\n",
    "embeddings = batch_embeddings(large_text_list)\n",
    "print(f\"–ü–æ–ª—É—á–µ–Ω–æ {len(embeddings)} –≤–µ–∫—Ç–æ—Ä–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cluster_documents(documents, n_clusters=3):\n",
    "    # –ü–æ–ª—É—á–∞–µ–º embeddings\n",
    "    embeddings = []\n",
    "    for doc in documents:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n",
    "            input=doc\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    \n",
    "    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    return clusters, embeddings\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ\n",
    "documents = [\n",
    "    \"–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Python\",\n",
    "    \"–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å JavaScript\", \n",
    "    \"–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å TensorFlow\",\n",
    "    \"–°–æ–∑–¥–∞–Ω–∏–µ API —Å FastAPI\",\n",
    "    \"Frontend —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å React\",\n",
    "    \"–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö —Å pandas\"\n",
    "]\n",
    "\n",
    "clusters, embeddings = cluster_documents(documents)\n",
    "\n",
    "for i, (doc, cluster) in enumerate(zip(documents, clusters)):\n",
    "    print(f\"–ö–ª–∞—Å—Ç–µ—Ä {cluster}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        \n",
    "    def add_documents(self, docs):\n",
    "        \"\"\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π\"\"\"\n",
    "        self.documents.extend(docs)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º embeddings\n",
    "        embeddings = []\n",
    "        for doc in docs:\n",
    "            response = client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=doc\n",
    "            )\n",
    "            embeddings.append(response.data[0].embedding)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å\n",
    "        dimension = len(embeddings[0])\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatIP(dimension)  # Inner Product\n",
    "        \n",
    "        embeddings_array = np.array(embeddings).astype('float32')\n",
    "        self.index.add(embeddings_array)\n",
    "    \n",
    "    def search(self, query, k=3):\n",
    "        \"\"\"–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\"\"\"\n",
    "        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "        query_response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        )\n",
    "        query_embedding = np.array([query_response.data[0].embedding]).astype('float32')\n",
    "        \n",
    "        # –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.documents):\n",
    "                results.append((self.documents[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"RAG: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\"\"\"\n",
    "        # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
    "        relevant_docs = self.search(question, k=2)\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "        context = \"\\n\".join([doc for doc, _ in relevant_docs])\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"–û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:\\n{context}\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAG\n",
    "rag = SimpleRAG()\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π\n",
    "knowledge_base = [\n",
    "    \"Cynosure Bridge - —ç—Ç–æ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –ø—Ä–æ–∫—Å–∏ –¥–ª—è Claude\",\n",
    "    \"Bridge —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø–æ—Ä—Ç—É 3000 –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç streaming\",\n",
    "    \"–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π OpenAI SDK, –ø—Ä–æ—Å—Ç–æ –ø–æ–º–µ–Ω—è–≤ base URL\",\n",
    "    \"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è embeddings —á–µ—Ä–µ–∑ /v1/embeddings endpoint\"\n",
    "]\n",
    "\n",
    "rag.add_documents(knowledge_base)\n",
    "\n",
    "# –ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å\n",
    "answer = rag.ask(\"–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Cynosure Bridge?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "class EmbeddingCache:\n",
    "    def __init__(self, cache_file=\"embeddings_cache.pkl\"):\n",
    "        self.cache_file = cache_file\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.cache = {}\n",
    "    \n",
    "    def get_embedding(self, text, model=\"text-embedding-3-small\"):\n",
    "        # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∫—ç—à–∞\n",
    "        key = hashlib.md5(f\"{model}:{text}\".encode()).hexdigest()\n",
    "        \n",
    "        if key in self.cache:\n",
    "            print(\"Cache hit!\")\n",
    "            return self.cache[key]\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º embedding –æ—Ç API\n",
    "        response = client.embeddings.create(model=model, input=text)\n",
    "        embedding = response.data[0].embedding\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à\n",
    "        self.cache[key] = embedding\n",
    "        self.save_cache()\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def save_cache(self):\n",
    "        with open(self.cache_file, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "cache = EmbeddingCache()\n",
    "\n",
    "# –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ - –±—É–¥–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ API\n",
    "start = time.time()\n",
    "embedding1 = cache.get_embedding(\"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\")\n",
    "time1 = time.time() - start\n",
    "\n",
    "# –í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ - –∏–∑ –∫—ç—à–∞\n",
    "start = time.time()\n",
    "embedding2 = cache.get_embedding(\"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞\")\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"–ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å: {time1:.3f}—Å\")\n",
    "print(f\"–í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å (–∫—ç—à): {time2:.3f}—Å\")\n",
    "print(f\"–£—Å–∫–æ—Ä–µ–Ω–∏–µ: {time1/time2:.1f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}