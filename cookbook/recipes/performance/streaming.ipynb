{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåä Streaming Performance —Å Cynosure Bridge\n",
    "\n",
    "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ streaming –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain-openai asyncio aiohttp websockets streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ –ë–∞–∑–æ–≤—ã–π streaming setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import asyncio\n",
    "import time\n",
    "from typing import AsyncGenerator, Generator\n",
    "import json\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://192.168.1.196:3000/v1\",\n",
    "    api_key=\"dummy-key\"\n",
    ")\n",
    "\n",
    "async_client = openai.AsyncOpenAI(\n",
    "    base_url=\"http://192.168.1.196:3000/v1\",\n",
    "    api_key=\"dummy-key\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è streaming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_streaming_chat(message: str, model: str = \"gpt-4\") -> Generator[str, None, None]:\n",
    "    \"\"\"–ë–∞–∑–æ–≤—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π streaming\"\"\"\n",
    "    try:\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            stream=True,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                yield chunk.choices[0].delta.content\n",
    "                \n",
    "    except Exception as e:\n",
    "        yield f\"–û—à–∏–±–∫–∞: {str(e)}\"\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ streaming\n",
    "print(\"üåä –ë–∞–∑–æ–≤—ã–π streaming:\")\n",
    "start_time = time.time()\n",
    "full_response = \"\"\n",
    "\n",
    "for chunk in basic_streaming_chat(\"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ streaming –≤ AI –º–æ–¥–µ–ª—è—Ö\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    full_response += chunk\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n\\n‚è±Ô∏è –í—Ä–µ–º—è: {end_time - start_time:.2f}—Å\")\n",
    "print(f\"üìù –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(full_response)} —Å–∏–º–≤–æ–ª–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_streaming_chat(message: str, model: str = \"gpt-4\") -> AsyncGenerator[str, None]:\n",
    "    \"\"\"–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π streaming –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\"\"\"\n",
    "    try:\n",
    "        stream = await async_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            stream=True,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        async for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                yield chunk.choices[0].delta.content\n",
    "                \n",
    "    except Exception as e:\n",
    "        yield f\"–û—à–∏–±–∫–∞: {str(e)}\"\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ streaming\n",
    "async def test_async_streaming():\n",
    "    print(\"üöÄ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π streaming:\")\n",
    "    start_time = time.time()\n",
    "    full_response = \"\"\n",
    "    \n",
    "    async for chunk in async_streaming_chat(\"–û–±—ä—è—Å–Ω–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è\"):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        full_response += chunk\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n\\n‚è±Ô∏è –í—Ä–µ–º—è: {end_time - start_time:.2f}—Å\")\n",
    "    print(f\"üìù –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(full_response)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "    return full_response\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞\n",
    "async_response = await test_async_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ streaming –∑–∞–ø—Ä–æ—Å—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parallel_streaming_requests(messages: list[str]) -> list[str]:\n",
    "    \"\"\"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö streaming –∑–∞–ø—Ä–æ—Å–æ–≤\"\"\"\n",
    "    async def process_single_request(message: str, request_id: int) -> str:\n",
    "        print(f\"\\nüîÑ –ó–∞–ø—Ä–æ—Å {request_id}: {message[:50]}...\")\n",
    "        full_response = \"\"\n",
    "        \n",
    "        async for chunk in async_streaming_chat(message):\n",
    "            full_response += chunk\n",
    "        \n",
    "        print(f\"\\n‚úÖ –ó–∞–ø—Ä–æ—Å {request_id} –∑–∞–≤–µ—Ä—à–µ–Ω ({len(full_response)} —Å–∏–º–≤–æ–ª–æ–≤)\")\n",
    "        return full_response\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "    tasks = [\n",
    "        process_single_request(message, i+1) \n",
    "        for i, message in enumerate(messages)\n",
    "    ]\n",
    "    \n",
    "    # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á\n",
    "    start_time = time.time()\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {end_time - start_time:.2f}—Å\")\n",
    "    return results\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "test_messages = [\n",
    "    \"–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?\",\n",
    "    \"–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏\",\n",
    "    \"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö\"\n",
    "]\n",
    "\n",
    "parallel_results = await parallel_streaming_requests(test_messages)\n",
    "print(f\"\\nüìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(parallel_results)} –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Streaming —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "class StreamingLogger:\n",
    "    def __init__(self, log_file: str = \"streaming_log.jsonl\"):\n",
    "        self.log_file = log_file\n",
    "        self.session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    async def streaming_with_logging(self, message: str, model: str = \"gpt-4\"):\n",
    "        \"\"\"Streaming —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞\"\"\"\n",
    "        session_data = {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"message\": message,\n",
    "            \"model\": model,\n",
    "            \"chunks\": []\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        full_response = \"\"\n",
    "        chunk_count = 0\n",
    "        \n",
    "        try:\n",
    "            async for chunk in async_streaming_chat(message, model):\n",
    "                chunk_time = time.time() - start_time\n",
    "                \n",
    "                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–∞\n",
    "                chunk_data = {\n",
    "                    \"chunk_id\": chunk_count,\n",
    "                    \"content\": chunk,\n",
    "                    \"timestamp\": chunk_time,\n",
    "                    \"content_length\": len(chunk)\n",
    "                }\n",
    "                session_data[\"chunks\"].append(chunk_data)\n",
    "                \n",
    "                full_response += chunk\n",
    "                chunk_count += 1\n",
    "                \n",
    "                # –í—ã–≤–æ–¥ —á–∞–Ω–∫–∞\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "                \n",
    "                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–∞–∂–¥—ã–µ 10 —á–∞–Ω–∫–æ–≤\n",
    "                if chunk_count % 10 == 0:\n",
    "                    self._save_session(session_data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            session_data[\"error\"] = str(e)\n",
    "        \n",
    "        finally:\n",
    "            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "            session_data[\"total_time\"] = time.time() - start_time\n",
    "            session_data[\"total_chunks\"] = chunk_count\n",
    "            session_data[\"full_response\"] = full_response\n",
    "            session_data[\"response_length\"] = len(full_response)\n",
    "            \n",
    "            self._save_session(session_data)\n",
    "            \n",
    "            print(f\"\\n\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "            print(f\"   ‚è±Ô∏è –í—Ä–µ–º—è: {session_data['total_time']:.2f}—Å\")\n",
    "            print(f\"   üß© –ß–∞–Ω–∫–æ–≤: {chunk_count}\")\n",
    "            print(f\"   üìù –°–∏–º–≤–æ–ª–æ–≤: {len(full_response)}\")\n",
    "            print(f\"   üíæ –õ–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {self.log_file}\")\n",
    "        \n",
    "        return full_response\n",
    "    \n",
    "    def _save_session(self, session_data: dict):\n",
    "        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–∏ –≤ JSONL —Ñ–∞–π–ª\"\"\"\n",
    "        with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(session_data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ streaming —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "logger = StreamingLogger()\n",
    "logged_response = await logger.streaming_with_logging(\n",
    "    \"–ù–∞–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç—å—é –ø—Ä–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ –µ–≥–æ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –æ–±—â–µ—Å—Ç–≤–æ\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ streaming –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedStreaming:\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"response_times\": [],\n",
    "            \"chunk_counts\": [],\n",
    "            \"tokens_per_second\": []\n",
    "        }\n",
    "    \n",
    "    async def optimized_stream(self, \n",
    "                              message: str, \n",
    "                              model: str = \"gpt-4\",\n",
    "                              max_tokens: int = 1000,\n",
    "                              temperature: float = 0.7,\n",
    "                              top_p: float = 1.0) -> str:\n",
    "        \"\"\"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π streaming —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\"\"\"\n",
    "        start_time = time.time()\n",
    "        full_response = \"\"\n",
    "        chunk_count = 0\n",
    "        \n",
    "        try:\n",
    "            stream = await async_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                stream=True,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p\n",
    "            )\n",
    "            \n",
    "            async for chunk in stream:\n",
    "                if chunk.choices[0].delta.content:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    full_response += content\n",
    "                    chunk_count += 1\n",
    "                    \n",
    "                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "                    if chunk_count % 3 == 0:  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–π 3-–π —á–∞–Ω–∫\n",
    "                        print(content, end=\"\", flush=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n–û—à–∏–±–∫–∞: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "        # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫\n",
    "        total_time = time.time() - start_time\n",
    "        tokens_per_sec = len(full_response.split()) / total_time if total_time > 0 else 0\n",
    "        \n",
    "        self.metrics[\"response_times\"].append(total_time)\n",
    "        self.metrics[\"chunk_counts\"].append(chunk_count)\n",
    "        self.metrics[\"tokens_per_second\"].append(tokens_per_sec)\n",
    "        \n",
    "        print(f\"\\n\\n‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\")\n",
    "        print(f\"   üìä {tokens_per_sec:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "        print(f\"   üß© {chunk_count} —á–∞–Ω–∫–æ–≤ –∑–∞ {total_time:.2f}—Å\")\n",
    "        \n",
    "        return full_response\n",
    "    \n",
    "    def get_performance_stats(self):\n",
    "        \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\"\"\"\n",
    "        if not self.metrics[\"response_times\"]:\n",
    "            return \"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\"\n",
    "        \n",
    "        import statistics\n",
    "        \n",
    "        stats = {\n",
    "            \"avg_response_time\": statistics.mean(self.metrics[\"response_times\"]),\n",
    "            \"avg_chunks\": statistics.mean(self.metrics[\"chunk_counts\"]),\n",
    "            \"avg_tokens_per_sec\": statistics.mean(self.metrics[\"tokens_per_second\"]),\n",
    "            \"total_requests\": len(self.metrics[\"response_times\"])\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ streaming\n",
    "optimizer = OptimizedStreaming()\n",
    "\n",
    "test_configs = [\n",
    "    {\"temperature\": 0.1, \"max_tokens\": 500, \"message\": \"–ö—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω–∏ –∫–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è\"},\n",
    "    {\"temperature\": 0.7, \"max_tokens\": 800, \"message\": \"–ù–∞–ø–∏—à–∏ —Ç–≤–æ—Ä—á–µ—Å–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ —Ä–æ–±–æ—Ç–æ–≤\"},\n",
    "    {\"temperature\": 0.3, \"max_tokens\": 600, \"message\": \"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –±–ª–æ–∫—á–µ–π–Ω —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏\"}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(test_configs, 1):\n",
    "    print(f\"\\nüß™ –¢–µ—Å—Ç {i}: temp={config['temperature']}, tokens={config['max_tokens']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    response = await optimizer.optimized_stream(\n",
    "        message=config[\"message\"],\n",
    "        temperature=config[\"temperature\"],\n",
    "        max_tokens=config[\"max_tokens\"]\n",
    "    )\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "final_stats = optimizer.get_performance_stats()\n",
    "print(f\"\\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
    "print(f\"   üî¢ –ó–∞–ø—Ä–æ—Å–æ–≤: {final_stats['total_requests']}\")\n",
    "print(f\"   ‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {final_stats['avg_response_time']:.2f}—Å\")\n",
    "print(f\"   üß© –°—Ä–µ–¥–Ω–∏–µ —á–∞–Ω–∫–∏: {final_stats['avg_chunks']:.1f}\")\n",
    "print(f\"   ‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {final_stats['avg_tokens_per_sec']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê WebSocket streaming (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import websockets\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "class WebSocketStreaming:\n",
    "    def __init__(self, host: str = \"localhost\", port: int = 8765):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.clients = set()\n",
    "    \n",
    "    async def websocket_handler(self, websocket, path):\n",
    "        \"\"\"–û–±—Ä–∞–±–æ—Ç—á–∏–∫ WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π\"\"\"\n",
    "        self.clients.add(websocket)\n",
    "        print(f\"üîå –ù–æ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ: {websocket.remote_address}\")\n",
    "        \n",
    "        try:\n",
    "            async for message in websocket:\n",
    "                data = json.loads(message)\n",
    "                \n",
    "                if data[\"type\"] == \"chat_request\":\n",
    "                    await self.handle_chat_request(websocket, data)\n",
    "                    \n",
    "        except websockets.exceptions.ConnectionClosed:\n",
    "            pass\n",
    "        finally:\n",
    "            self.clients.remove(websocket)\n",
    "            print(f\"üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–æ: {websocket.remote_address}\")\n",
    "    \n",
    "    async def handle_chat_request(self, websocket, data):\n",
    "        \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ chat –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ WebSocket\"\"\"\n",
    "        message = data.get(\"message\", \"\")\n",
    "        session_id = data.get(\"session_id\", \"default\")\n",
    "        \n",
    "        # –û—Ç–ø—Ä–∞–≤–∫–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
    "        await websocket.send(json.dumps({\n",
    "            \"type\": \"stream_start\",\n",
    "            \"session_id\": session_id,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }))\n",
    "        \n",
    "        try:\n",
    "            # Streaming –æ—Ç–≤–µ—Ç\n",
    "            async for chunk in async_streaming_chat(message):\n",
    "                await websocket.send(json.dumps({\n",
    "                    \"type\": \"stream_chunk\",\n",
    "                    \"session_id\": session_id,\n",
    "                    \"content\": chunk,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }))\n",
    "            \n",
    "            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å—Ç—Ä–∏–º–∞\n",
    "            await websocket.send(json.dumps({\n",
    "                \"type\": \"stream_end\",\n",
    "                \"session_id\": session_id,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }))\n",
    "            \n",
    "        except Exception as e:\n",
    "            await websocket.send(json.dumps({\n",
    "                \"type\": \"error\",\n",
    "                \"session_id\": session_id,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }))\n",
    "    \n",
    "    async def start_server(self):\n",
    "        \"\"\"–ó–∞–ø—É—Å–∫ WebSocket —Å–µ—Ä–≤–µ—Ä–∞\"\"\"\n",
    "        print(f\"üöÄ WebSocket —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞ ws://{self.host}:{self.port}\")\n",
    "        \n",
    "        async with websockets.serve(self.websocket_handler, self.host, self.port):\n",
    "            print(\"‚úÖ WebSocket —Å–µ—Ä–≤–µ—Ä –∞–∫—Ç–∏–≤–µ–Ω\")\n",
    "            await asyncio.Future()  # –ó–∞–ø—É—Å–∫ –Ω–∞–≤—Å–µ–≥–¥–∞\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "async def websocket_client_test():\n",
    "    \"\"\"–¢–µ—Å—Ç–æ–≤—ã–π WebSocket –∫–ª–∏–µ–Ω—Ç\"\"\"\n",
    "    uri = \"ws://localhost:8765\"\n",
    "    \n",
    "    try:\n",
    "        async with websockets.connect(uri) as websocket:\n",
    "            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞\n",
    "            request = {\n",
    "                \"type\": \"chat_request\",\n",
    "                \"message\": \"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ WebSocket –ø—Ä–æ—Ç–æ–∫–æ–ª\",\n",
    "                \"session_id\": \"test_session\"\n",
    "            }\n",
    "            \n",
    "            await websocket.send(json.dumps(request))\n",
    "            \n",
    "            # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤\n",
    "            async for message in websocket:\n",
    "                data = json.loads(message)\n",
    "                \n",
    "                if data[\"type\"] == \"stream_start\":\n",
    "                    print(\"üåä –ù–∞—á–∞–ª–æ streaming...\")\n",
    "                elif data[\"type\"] == \"stream_chunk\":\n",
    "                    print(data[\"content\"], end=\"\", flush=True)\n",
    "                elif data[\"type\"] == \"stream_end\":\n",
    "                    print(\"\\n‚úÖ Streaming –∑–∞–≤–µ—Ä—à–µ–Ω\")\n",
    "                    break\n",
    "                elif data[\"type\"] == \"error\":\n",
    "                    print(f\"\\n‚ùå –û—à–∏–±–∫–∞: {data['error']}\")\n",
    "                    break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}\")\n",
    "\n",
    "print(\"üåê WebSocket streaming –∫–ª–∞—Å—Å –≥–æ—Ç–æ–≤\")\n",
    "print(\"üí° –î–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:\")\n",
    "print(\"   ws_server = WebSocketStreaming()\")\n",
    "print(\"   await ws_server.start_server()\")\n",
    "print(\"üí° –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞:\")\n",
    "print(\"   await websocket_client_test()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Benchmarking –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statistics import mean, median, stdev\n",
    "\n",
    "class StreamingBenchmark:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    async def benchmark_streaming_methods(self, test_message: str, iterations: int = 3):\n",
    "        \"\"\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ streaming\"\"\"\n",
    "        methods = {\n",
    "            \"sync_basic\": self.test_sync_streaming,\n",
    "            \"async_basic\": self.test_async_streaming,\n",
    "            \"async_optimized\": self.test_optimized_streaming\n",
    "        }\n",
    "        \n",
    "        print(f\"üß™ Benchmark: {iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞\")\n",
    "        print(f\"üìù –¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {test_message[:50]}...\\n\")\n",
    "        \n",
    "        for method_name, method_func in methods.items():\n",
    "            print(f\"üî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {method_name}...\")\n",
    "            \n",
    "            method_results = []\n",
    "            for i in range(iterations):\n",
    "                start_time = time.time()\n",
    "                response = await method_func(test_message)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                result = {\n",
    "                    \"method\": method_name,\n",
    "                    \"iteration\": i + 1,\n",
    "                    \"response_time\": end_time - start_time,\n",
    "                    \"response_length\": len(response),\n",
    "                    \"tokens_per_second\": len(response.split()) / (end_time - start_time)\n",
    "                }\n",
    "                \n",
    "                method_results.append(result)\n",
    "                self.results.append(result)\n",
    "                \n",
    "                print(f\"  ‚úÖ –ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}: {result['response_time']:.2f}—Å, {result['tokens_per_second']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\")\n",
    "            \n",
    "            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–æ–¥—É\n",
    "            avg_time = mean([r[\"response_time\"] for r in method_results])\n",
    "            avg_tokens_sec = mean([r[\"tokens_per_second\"] for r in method_results])\n",
    "            \n",
    "            print(f\"üìä {method_name} - —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.2f}—Å, —Å–∫–æ—Ä–æ—Å—Ç—å: {avg_tokens_sec:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\\n\")\n",
    "    \n",
    "    async def test_sync_streaming(self, message: str) -> str:\n",
    "        \"\"\"–¢–µ—Å—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ streaming\"\"\"\n",
    "        response = \"\"\n",
    "        for chunk in basic_streaming_chat(message):\n",
    "            response += chunk\n",
    "        return response\n",
    "    \n",
    "    async def test_async_streaming(self, message: str) -> str:\n",
    "        \"\"\"–¢–µ—Å—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ streaming\"\"\"\n",
    "        response = \"\"\n",
    "        async for chunk in async_streaming_chat(message):\n",
    "            response += chunk\n",
    "        return response\n",
    "    \n",
    "    async def test_optimized_streaming(self, message: str) -> str:\n",
    "        \"\"\"–¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ streaming\"\"\"\n",
    "        optimizer = OptimizedStreaming()\n",
    "        return await optimizer.optimized_stream(message, temperature=0.3, max_tokens=600)\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á–µ—Ç–∞\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        print(\"üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ BENCHMARK\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º\n",
    "        summary = df.groupby('method').agg({\n",
    "            'response_time': ['mean', 'median', 'std'],\n",
    "            'tokens_per_second': ['mean', 'median', 'std'],\n",
    "            'response_length': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        print(summary)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # –ì—Ä–∞—Ñ–∏–∫ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞\n",
    "        methods = df['method'].unique()\n",
    "        response_times = [df[df['method'] == method]['response_time'].values for method in methods]\n",
    "        \n",
    "        ax1.boxplot(response_times, labels=methods)\n",
    "        ax1.set_title('–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º')\n",
    "        ax1.set_ylabel('–°–µ–∫—É–Ω–¥—ã')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É\n",
    "        tokens_per_sec = [df[df['method'] == method]['tokens_per_second'].values for method in methods]\n",
    "        \n",
    "        ax2.boxplot(tokens_per_sec, labels=methods)\n",
    "        ax2.set_title('–¢–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –ø–æ –º–µ—Ç–æ–¥–∞–º')\n",
    "        ax2.set_ylabel('–¢–æ–∫–µ–Ω—ã/—Å–µ–∫')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "        best_speed = df.loc[df['tokens_per_second'].idxmax()]\n",
    "        best_time = df.loc[df['response_time'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nüèÜ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
    "        print(f\"‚ö° –õ—É—á—à–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: {best_speed['method']} ({best_speed['tokens_per_second']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)\")\n",
    "        print(f\"üèÉ –õ—É—á—à–µ–µ –≤—Ä–µ–º—è: {best_time['method']} ({best_time['response_time']:.2f}—Å)\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ benchmark\n",
    "benchmark = StreamingBenchmark()\n",
    "await benchmark.benchmark_streaming_methods(\n",
    "    \"–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã REST API –∏ –ø—Ä–∏–≤–µ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\",\n",
    "    iterations=2  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    ")\n",
    "\n",
    "benchmark.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "\n",
    "### ‚úÖ –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ streaming:\n",
    "\n",
    "1. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–¥** –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "2. **–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —á–∞—Å—Ç–æ—Ç—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π** –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞\n",
    "3. **–õ–æ–≥–∏—Ä—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "4. **–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π—Ç–µ –æ—à–∏–±–∫–∏** gracefully\n",
    "5. **–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã** –º–æ–¥–µ–ª–∏\n",
    "\n",
    "### üöÄ –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞:\n",
    "\n",
    "- **WebSocket** –¥–ª—è real-time –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π\n",
    "- **–ë—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è** –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "- **Rate limiting** –¥–ª—è –∑–∞—â–∏—Ç—ã\n",
    "- **Monitoring** –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "### üìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è:\n",
    "\n",
    "- Time to First Token (TTFT)\n",
    "- Tokens per Second\n",
    "- Total Response Time\n",
    "- Error Rate\n",
    "- Concurrent Connections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}