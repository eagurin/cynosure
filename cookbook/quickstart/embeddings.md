# üîç Python Embeddings —á–µ—Ä–µ–∑ Cynosure Bridge

–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ embedding —á–µ—Ä–µ–∑ OpenAI API —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å.

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
import openai
import numpy as np

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Cynosure Bridge
client = openai.OpenAI(
    base_url="http://192.168.1.196:3000/v1",
    api_key="dummy-key"
)

# –°–æ–∑–¥–∞–Ω–∏–µ embedding
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"
)

embedding = response.data[0].embedding
print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞: {len(embedding)}")
print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {response.usage.total_tokens}")
```

## üìä –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏

| OpenAI Model | Claude Alternative | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ |
|--------------|-------------------|-------------|------------|
| `text-embedding-3-small` | `claude-3-5-sonnet-20241022` | 1536 | –ë—ã—Å—Ç—Ä–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è |
| `text-embedding-3-large` | `claude-3-5-sonnet-20241022` | 3072 | –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ |
| `text-embedding-ada-002` | `claude-3-5-haiku-20241022` | 1536 | –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å |

## üí° –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
```python
def semantic_search(query, documents):
    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
    query_response = client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    )
    query_embedding = query_response.data[0].embedding
    
    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    doc_embeddings = []
    for doc in documents:
        doc_response = client.embeddings.create(
            model="text-embedding-3-small",
            input=doc
        )
        doc_embeddings.append(doc_response.data[0].embedding)
    
    # –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–µ–≥–æ
    similarities = [
        np.dot(query_embedding, doc_emb) for doc_emb in doc_embeddings
    ]
    
    best_match_idx = np.argmax(similarities)
    return documents[best_match_idx], similarities[best_match_idx]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
documents = [
    "Python - —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è",
    "JavaScript –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", 
    "Claude - —ç—Ç–æ AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –æ—Ç Anthropic"
]

result, score = semantic_search("–ß—Ç–æ —Ç–∞–∫–æ–µ Python?", documents)
print(f"–ù–∞–π–¥–µ–Ω–æ: {result} (score: {score:.3f})")
```

### 2. Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞
```python
def batch_embeddings(texts, batch_size=100):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤ —Ç–µ–∫—Å—Ç–∞ –ø–∞–∫–µ—Ç–∞–º–∏"""
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        
        batch_embeddings = [item.embedding for item in response.data]
        all_embeddings.extend(batch_embeddings)
        
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i + batch_size, len(texts))} –∏–∑ {len(texts)}")
    
    return all_embeddings

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
large_text_list = ["–¢–µ–∫—Å—Ç " + str(i) for i in range(500)]
embeddings = batch_embeddings(large_text_list)
```

### 3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

def cluster_documents(documents, n_clusters=3):
    # –ü–æ–ª—É—á–∞–µ–º embeddings
    embeddings = []
    for doc in documents:
        response = client.embeddings.create(
            model="text-embedding-3-large",  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            input=doc
        )
        embeddings.append(response.data[0].embedding)
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(embeddings)
    
    return clusters, embeddings

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
documents = [
    "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Python",
    "–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å JavaScript", 
    "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å TensorFlow",
    "–°–æ–∑–¥–∞–Ω–∏–µ API —Å FastAPI",
    "Frontend —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å React",
    "–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö —Å pandas"
]

clusters, embeddings = cluster_documents(documents)

for i, (doc, cluster) in enumerate(zip(documents, clusters)):
    print(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster}: {doc}")
```

### 4. RAG (Retrieval Augmented Generation)
```python
import faiss
import numpy as np

class SimpleRAG:
    def __init__(self):
        self.documents = []
        self.index = None
        
    def add_documents(self, docs):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        self.documents.extend(docs)
        
        # –°–æ–∑–¥–∞–µ–º embeddings
        embeddings = []
        for doc in docs:
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=doc
            )
            embeddings.append(response.data[0].embedding)
        
        # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        dimension = len(embeddings[0])
        if self.index is None:
            self.index = faiss.IndexFlatIP(dimension)  # Inner Product
        
        embeddings_array = np.array(embeddings).astype('float32')
        self.index.add(embeddings_array)
    
    def search(self, query, k=3):
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
        query_response = client.embeddings.create(
            model="text-embedding-3-small",
            input=query
        )
        query_embedding = np.array([query_response.data[0].embedding]).astype('float32')
        
        # –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ
        scores, indices = self.index.search(query_embedding, k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                results.append((self.documents[idx], float(score)))
        
        return results
    
    def ask(self, question):
        """RAG: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        relevant_docs = self.search(question, k=2)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = "\n".join([doc for doc, _ in relevant_docs])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": f"–û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:\n{context}"},
                {"role": "user", "content": question}
            ]
        )
        
        return response.choices[0].message.content

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAG
rag = SimpleRAG()

# –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
knowledge_base = [
    "Cynosure Bridge - —ç—Ç–æ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –ø—Ä–æ–∫—Å–∏ –¥–ª—è Claude",
    "Bridge —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ø–æ—Ä—Ç—É 3000 –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç streaming",
    "–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π OpenAI SDK, –ø—Ä–æ—Å—Ç–æ –ø–æ–º–µ–Ω—è–≤ base URL",
    "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è embeddings —á–µ—Ä–µ–∑ /v1/embeddings endpoint"
]

rag.add_documents(knowledge_base)

# –ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å
answer = rag.ask("–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Cynosure Bridge?")
print(answer)
```

## üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ embeddings
```python
import pickle
import hashlib

class EmbeddingCache:
    def __init__(self, cache_file="embeddings_cache.pkl"):
        self.cache_file = cache_file
        try:
            with open(cache_file, 'rb') as f:
                self.cache = pickle.load(f)
        except FileNotFoundError:
            self.cache = {}
    
    def get_embedding(self, text, model="text-embedding-3-small"):
        # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∫—ç—à–∞
        key = hashlib.md5(f"{model}:{text}".encode()).hexdigest()
        
        if key in self.cache:
            return self.cache[key]
        
        # –ü–æ–ª—É—á–∞–µ–º embedding –æ—Ç API
        response = client.embeddings.create(model=model, input=text)
        embedding = response.data[0].embedding
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.cache[key] = embedding
        self.save_cache()
        
        return embedding
    
    def save_cache(self):
        with open(self.cache_file, 'wb') as f:
            pickle.dump(self.cache, f)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
cache = EmbeddingCache()
embedding = cache.get_embedding("–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞")
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏

```python
import time
from datetime import datetime

class EmbeddingMonitor:
    def __init__(self):
        self.stats = {
            'requests': 0,
            'total_tokens': 0,
            'total_time': 0,
            'errors': 0
        }
    
    def create_embedding(self, text, model="text-embedding-3-small"):
        start_time = time.time()
        
        try:
            response = client.embeddings.create(model=model, input=text)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats['requests'] += 1
            self.stats['total_tokens'] += response.usage.total_tokens
            self.stats['total_time'] += time.time() - start_time
            
            return response.data[0].embedding
            
        except Exception as e:
            self.stats['errors'] += 1
            print(f"–û—à–∏–±–∫–∞: {e}")
            return None
    
    def get_stats(self):
        if self.stats['requests'] > 0:
            avg_time = self.stats['total_time'] / self.stats['requests']
            avg_tokens = self.stats['total_tokens'] / self.stats['requests']
            
            return {
                'requests': self.stats['requests'],
                'errors': self.stats['errors'],
                'avg_response_time': f"{avg_time:.3f}s",
                'avg_tokens_per_request': f"{avg_tokens:.1f}",
                'total_tokens': self.stats['total_tokens']
            }
        return self.stats

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
monitor = EmbeddingMonitor()

# –ù–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤
texts = ["–¢–µ–∫—Å—Ç 1", "–¢–µ–∫—Å—Ç 2", "–¢–µ–∫—Å—Ç 3"]
for text in texts:
    monitor.create_embedding(text)

print(monitor.get_stats())
```

## üéØ –î–∞–ª—å–Ω–µ–π—à–∏–µ —à–∞–≥–∏

- **[Document Search](../../examples/embeddings/document-search.md)** - –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
- **[RAG Implementation](../../examples/embeddings/rag.md)** - –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è RAG
- **[Performance Tips](../../recipes/performance/batching.md)** - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏